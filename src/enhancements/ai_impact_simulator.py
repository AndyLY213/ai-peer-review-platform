"""
AIImpactSimulator module for modeling AI assistance in writing and reviewing academic papers.
"""

import json
import uuid
import random
from datetime import date, datetime, timedelta
from dataclasses import dataclass, asdict
from enum import Enum
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any, Set
import math

from src.core.exceptions import ValidationError, PeerReviewError


class AIAssistanceType(Enum):
    """Types of AI assistance in academic writing and reviewing."""
    WRITING_ASSISTANCE = "writing_assistance"
    GRAMMAR_CHECK = "grammar_check"
    LITERATURE_REVIEW = "literature_review"
    DATA_ANALYSIS = "data_analysis"
    FIGURE_GENERATION = "figure_generation"
    CITATION_MANAGEMENT = "citation_management"
    REVIEW_ASSISTANCE = "review_assistance"
    PEER_REVIEW_AUTOMATION = "peer_review_automation"
    PLAGIARISM_DETECTION = "plagiarism_detection"
    TRANSLATION = "translation"


class AIDetectionMethod(Enum):
    """Methods for detecting AI-generated content."""
    STATISTICAL_ANALYSIS = "statistical_analysis"
    LINGUISTIC_PATTERNS = "linguistic_patterns"
    WATERMARKING = "watermarking"
    HUMAN_REVIEW = "human_review"
    HYBRID_DETECTION = "hybrid_detection"
    NO_DETECTION = "no_detection"


class AIPolicy(Enum):
    """Institutional policies regarding AI use."""
    PROHIBITED = "prohibited"
    DISCLOSURE_REQUIRED = "disclosure_required"
    LIMITED_USE = "limited_use"
    UNRESTRICTED = "unrestricted"
    UNDER_REVIEW = "under_review"


class AIQualityImpact(Enum):
    """Impact of AI assistance on content quality."""
    SIGNIFICANT_IMPROVEMENT = "significant_improvement"
    MODERATE_IMPROVEMENT = "moderate_improvement"
    MINIMAL_IMPROVEMENT = "minimal_improvement"
    NO_CHANGE = "no_change"
    QUALITY_DEGRADATION = "quality_degradation"


@dataclass
class AIUsageRecord:
    """Represents AI usage in academic work."""
    usage_id: str
    researcher_id: str
    paper_id: Optional[str] = None
    review_id: Optional[str] = None
    assistance_type: AIAssistanceType = AIAssistanceType.WRITING_ASSISTANCE
    ai_tool_name: str = "Generic AI"
    usage_date: date = None
    usage_duration_minutes: int = 0
    content_percentage: float = 0.0  # Percentage of content generated by AI
    is_disclosed: bool = False
    quality_impact: AIQualityImpact = AIQualityImpact.NO_CHANGE
    detection_attempted: bool = False
    detected_as_ai: bool = False
    detection_confidence: float = 0.0
    
    def __post_init__(self):
        """Validate AI usage record data."""
        if self.usage_date is None:
            self.usage_date = date.today()
        if not (0.0 <= self.content_percentage <= 1.0):
            raise ValidationError("content_percentage", self.content_percentage, "between 0.0 and 1.0")
        if not (0.0 <= self.detection_confidence <= 1.0):
            raise ValidationError("detection_confidence", self.detection_confidence, "between 0.0 and 1.0")
        if self.usage_duration_minutes < 0:
            raise ValidationError("usage_duration_minutes", self.usage_duration_minutes, "non-negative")


@dataclass
class AIDetectionResult:
    """Results of AI detection analysis."""
    detection_id: str
    content_id: str  # paper_id or review_id
    detection_method: AIDetectionMethod
    detection_date: date
    ai_probability: float
    confidence_level: float
    flagged_sections: List[str]
    human_verification: Optional[bool] = None
    false_positive_risk: float = 0.0
    
    def __post_init__(self):
        """Validate AI detection result data."""
        if not (0.0 <= self.ai_probability <= 1.0):
            raise ValidationError("ai_probability", self.ai_probability, "between 0.0 and 1.0")
        if not (0.0 <= self.confidence_level <= 1.0):
            raise ValidationError("confidence_level", self.confidence_level, "between 0.0 and 1.0")
        if not (0.0 <= self.false_positive_risk <= 1.0):
            raise ValidationError("false_positive_risk", self.false_positive_risk, "between 0.0 and 1.0")


@dataclass
class AIPolicyRecord:
    """Represents institutional AI policies."""
    institution_id: str
    policy_type: AIPolicy
    effective_date: date
    policy_description: str
    allowed_tools: List[str]
    disclosure_requirements: List[str]
    enforcement_level: float  # 0.0 = no enforcement, 1.0 = strict enforcement
    violation_penalties: List[str]
    
    def __post_init__(self):
        """Validate AI policy record data."""
        if not (0.0 <= self.enforcement_level <= 1.0):
            raise ValidationError("enforcement_level", self.enforcement_level, "between 0.0 and 1.0")
        if not self.allowed_tools:
            self.allowed_tools = []
        if not self.disclosure_requirements:
            self.disclosure_requirements = []
        if not self.violation_penalties:
            self.violation_penalties = []


@dataclass
class AIAdoptionProfile:
    """Represents a researcher's AI adoption profile."""
    researcher_id: str
    adoption_rate: float
    preferred_tools: List[str]
    comfort_level: float
    ethical_concerns: float
    disclosure_compliance: float
    career_stage_influence: float
    field_influence: float
    institutional_policy_compliance: float
    
    def __post_init__(self):
        """Validate AI adoption profile data."""
        if not (0.0 <= self.adoption_rate <= 1.0):
            raise ValidationError("adoption_rate", self.adoption_rate, "between 0.0 and 1.0")
        if not (0.0 <= self.comfort_level <= 1.0):
            raise ValidationError("comfort_level", self.comfort_level, "between 0.0 and 1.0")
        if not (0.0 <= self.ethical_concerns <= 1.0):
            raise ValidationError("ethical_concerns", self.ethical_concerns, "between 0.0 and 1.0")
        if not (0.0 <= self.disclosure_compliance <= 1.0):
            raise ValidationError("disclosure_compliance", self.disclosure_compliance, "between 0.0 and 1.0")
        if not self.preferred_tools:
            self.preferred_tools = []


@dataclass
class AIImpactMetrics:
    """Aggregated metrics for AI impact on academic work."""
    total_ai_usage_instances: int
    ai_adoption_rate: float
    average_content_percentage: float
    disclosure_rate: float
    detection_accuracy: float
    quality_improvement_rate: float
    policy_compliance_rate: float
    tool_usage_distribution: Dict[str, int]
    assistance_type_distribution: Dict[AIAssistanceType, int]
    detection_method_effectiveness: Dict[AIDetectionMethod, float]
    
    def __post_init__(self):
        """Validate AI impact metrics."""
        if self.total_ai_usage_instances < 0:
            raise ValidationError("total_ai_usage_instances", self.total_ai_usage_instances, "non-negative")
        if not (0.0 <= self.ai_adoption_rate <= 1.0):
            raise ValidationError("ai_adoption_rate", self.ai_adoption_rate, "between 0.0 and 1.0")


class AIImpactSimulator:
    """
    Simulates the impact of AI assistance on academic writing and peer review processes.
    Models AI adoption, detection systems, and policy enforcement.
    """
    
    def __init__(self, data_dir: Optional[Path] = None):
        """Initialize the AIImpactSimulator."""
        self.data_dir = data_dir or Path("data/ai_impact")
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # Storage for AI impact records
        self.usage_records: Dict[str, AIUsageRecord] = {}
        self.detection_results: Dict[str, AIDetectionResult] = {}
        self.policy_records: Dict[str, AIPolicyRecord] = {}
        self.adoption_profiles: Dict[str, AIAdoptionProfile] = {}
        
        # Configuration
        self.tool_effectiveness = {
            "GPT-4": 0.85,
            "Claude": 0.82,
            "Grammarly": 0.75,
            "Writefull": 0.70,
            "Jasper": 0.65,
            "Copy.ai": 0.60,
            "Generic AI": 0.50
        }
        
        self.field_ai_adoption = {
            "computer_science": 0.8,
            "physics": 0.6,
            "biology": 0.5,
            "medicine": 0.4,
            "psychology": 0.5,
            "sociology": 0.3,
            "chemistry": 0.4,
            "mathematics": 0.7
        }
        
        # Load existing data
        self._load_data()
    
    def create_adoption_profile(self, researcher_id: str, career_stage: str, 
                              field: str, institution_id: str) -> AIAdoptionProfile:
        """Create an AI adoption profile for a researcher."""
        # Career stage influences adoption
        career_multipliers = {
            "Graduate Student": 0.9,  # High adoption, tech-savvy
            "Postdoc": 0.8,  # High adoption, efficiency focused
            "Assistant Prof": 0.7,  # Moderate adoption, cautious
            "Associate Prof": 0.5,  # Lower adoption, established methods
            "Full Prof": 0.4,  # Low adoption, traditional
            "Emeritus": 0.2   # Very low adoption
        }
        
        base_multiplier = career_multipliers.get(career_stage, 0.6)
        field_multiplier = self.field_ai_adoption.get(field.lower(), 0.5)
        
        # Calculate adoption metrics with randomness
        adoption_rate = min(1.0, base_multiplier * field_multiplier * random.uniform(0.7, 1.3))
        comfort_level = min(1.0, base_multiplier * random.uniform(0.8, 1.2))
        
        # Ethical concerns inversely related to adoption
        ethical_concerns = max(0.0, 1.0 - adoption_rate * random.uniform(0.8, 1.2))
        
        # Disclosure compliance varies by career stage
        disclosure_base = {
            "Graduate Student": 0.6,
            "Postdoc": 0.7,
            "Assistant Prof": 0.8,
            "Associate Prof": 0.9,
            "Full Prof": 0.95,
            "Emeritus": 0.98
        }
        disclosure_compliance = min(1.0, disclosure_base.get(career_stage, 0.8) * random.uniform(0.8, 1.1))
        
        # Get institutional policy compliance
        policy = self.policy_records.get(institution_id)
        policy_compliance = 0.8 if not policy else min(1.0, 0.5 + policy.enforcement_level * 0.5)
        
        # Select preferred tools based on field and career stage
        all_tools = list(self.tool_effectiveness.keys())
        num_tools = random.randint(1, min(3, len(all_tools)))
        preferred_tools = random.sample(all_tools, num_tools)
        
        profile = AIAdoptionProfile(
            researcher_id=researcher_id,
            adoption_rate=adoption_rate,
            preferred_tools=preferred_tools,
            comfort_level=comfort_level,
            ethical_concerns=ethical_concerns,
            disclosure_compliance=disclosure_compliance,
            career_stage_influence=base_multiplier,
            field_influence=field_multiplier,
            institutional_policy_compliance=policy_compliance
        )
        
        self.adoption_profiles[researcher_id] = profile
        return profile
    
    def simulate_ai_usage(self, researcher_id: str, paper_id: Optional[str] = None,
                         review_id: Optional[str] = None, 
                         assistance_type: AIAssistanceType = AIAssistanceType.WRITING_ASSISTANCE) -> Optional[AIUsageRecord]:
        """Simulate AI usage by a researcher."""
        profile = self.adoption_profiles.get(researcher_id)
        if not profile:
            # Create default profile if none exists
            profile = self.create_adoption_profile(researcher_id, "Assistant Prof", "computer_science", "default_institution")
        
        # Check if researcher uses AI
        if random.random() > profile.adoption_rate:
            return None
        
        # Select AI tool
        if profile.preferred_tools:
            tool_name = random.choice(profile.preferred_tools)
        else:
            tool_name = "Generic AI"
        
        # Determine usage characteristics
        tool_effectiveness = self.tool_effectiveness.get(tool_name, 0.5)
        
        # Content percentage varies by assistance type
        content_percentages = {
            AIAssistanceType.WRITING_ASSISTANCE: (0.1, 0.4),
            AIAssistanceType.GRAMMAR_CHECK: (0.05, 0.15),
            AIAssistanceType.LITERATURE_REVIEW: (0.2, 0.6),
            AIAssistanceType.DATA_ANALYSIS: (0.1, 0.3),
            AIAssistanceType.FIGURE_GENERATION: (0.05, 0.2),
            AIAssistanceType.CITATION_MANAGEMENT: (0.02, 0.1),
            AIAssistanceType.REVIEW_ASSISTANCE: (0.15, 0.5),
            AIAssistanceType.PEER_REVIEW_AUTOMATION: (0.3, 0.8),
            AIAssistanceType.PLAGIARISM_DETECTION: (0.0, 0.05),
            AIAssistanceType.TRANSLATION: (0.1, 0.9)
        }
        
        min_pct, max_pct = content_percentages.get(assistance_type, (0.1, 0.3))
        content_percentage = random.uniform(min_pct, max_pct)
        
        # Usage duration varies by type
        duration_ranges = {
            AIAssistanceType.WRITING_ASSISTANCE: (30, 180),
            AIAssistanceType.GRAMMAR_CHECK: (5, 30),
            AIAssistanceType.LITERATURE_REVIEW: (60, 300),
            AIAssistanceType.DATA_ANALYSIS: (45, 240),
            AIAssistanceType.FIGURE_GENERATION: (15, 90),
            AIAssistanceType.CITATION_MANAGEMENT: (10, 60),
            AIAssistanceType.REVIEW_ASSISTANCE: (20, 120),
            AIAssistanceType.PEER_REVIEW_AUTOMATION: (5, 30),
            AIAssistanceType.PLAGIARISM_DETECTION: (2, 15),
            AIAssistanceType.TRANSLATION: (30, 180)
        }
        
        min_dur, max_dur = duration_ranges.get(assistance_type, (15, 90))
        usage_duration = random.randint(min_dur, max_dur)
        
        # Determine disclosure
        is_disclosed = random.random() < profile.disclosure_compliance
        
        # Determine quality impact
        quality_impacts = [
            AIQualityImpact.SIGNIFICANT_IMPROVEMENT,
            AIQualityImpact.MODERATE_IMPROVEMENT,
            AIQualityImpact.MINIMAL_IMPROVEMENT,
            AIQualityImpact.NO_CHANGE,
            AIQualityImpact.QUALITY_DEGRADATION
        ]
        
        # Higher effectiveness tools more likely to improve quality
        improvement_prob = tool_effectiveness * profile.comfort_level
        if improvement_prob > 0.8:
            quality_impact = AIQualityImpact.SIGNIFICANT_IMPROVEMENT
        elif improvement_prob > 0.6:
            quality_impact = AIQualityImpact.MODERATE_IMPROVEMENT
        elif improvement_prob > 0.4:
            quality_impact = AIQualityImpact.MINIMAL_IMPROVEMENT
        elif improvement_prob > 0.2:
            quality_impact = AIQualityImpact.NO_CHANGE
        else:
            quality_impact = AIQualityImpact.QUALITY_DEGRADATION
        
        usage_record = AIUsageRecord(
            usage_id=str(uuid.uuid4()),
            researcher_id=researcher_id,
            paper_id=paper_id,
            review_id=review_id,
            assistance_type=assistance_type,
            ai_tool_name=tool_name,
            usage_date=date.today(),
            usage_duration_minutes=usage_duration,
            content_percentage=content_percentage,
            is_disclosed=is_disclosed,
            quality_impact=quality_impact
        )
        
        self.usage_records[usage_record.usage_id] = usage_record
        return usage_record
    
    def detect_ai_content(self, content_id: str, detection_method: AIDetectionMethod,
                         actual_ai_usage: Optional[AIUsageRecord] = None) -> AIDetectionResult:
        """Simulate AI content detection."""
        # Detection accuracy varies by method
        method_accuracy = {
            AIDetectionMethod.STATISTICAL_ANALYSIS: 0.75,
            AIDetectionMethod.LINGUISTIC_PATTERNS: 0.70,
            AIDetectionMethod.WATERMARKING: 0.95,
            AIDetectionMethod.HUMAN_REVIEW: 0.85,
            AIDetectionMethod.HYBRID_DETECTION: 0.88,
            AIDetectionMethod.NO_DETECTION: 0.0
        }
        
        base_accuracy = method_accuracy.get(detection_method, 0.7)
        
        # Determine if content actually contains AI
        has_ai_content = actual_ai_usage is not None
        ai_content_percentage = actual_ai_usage.content_percentage if has_ai_content else 0.0
        
        # Detection probability increases with AI content percentage
        detection_probability = base_accuracy * (ai_content_percentage ** 0.5) if has_ai_content else (1 - base_accuracy)
        
        # Add noise to detection
        noise = random.uniform(-0.1, 0.1)
        ai_probability = max(0.0, min(1.0, detection_probability + noise))
        
        # Confidence varies by method and result clarity
        confidence_base = {
            AIDetectionMethod.STATISTICAL_ANALYSIS: 0.6,
            AIDetectionMethod.LINGUISTIC_PATTERNS: 0.65,
            AIDetectionMethod.WATERMARKING: 0.95,
            AIDetectionMethod.HUMAN_REVIEW: 0.8,
            AIDetectionMethod.HYBRID_DETECTION: 0.85,
            AIDetectionMethod.NO_DETECTION: 0.0
        }
        
        base_confidence = confidence_base.get(detection_method, 0.7)
        confidence_level = base_confidence * random.uniform(0.8, 1.2)
        confidence_level = max(0.0, min(1.0, confidence_level))
        
        # Generate flagged sections if AI detected
        flagged_sections = []
        if ai_probability > 0.5:
            section_types = ["abstract", "introduction", "methodology", "results", "discussion", "conclusion"]
            num_flagged = random.randint(1, min(3, len(section_types)))
            flagged_sections = random.sample(section_types, num_flagged)
        
        # Calculate false positive risk
        false_positive_risk = max(0.0, (1 - base_accuracy) * (1 - ai_content_percentage))
        
        detection_result = AIDetectionResult(
            detection_id=str(uuid.uuid4()),
            content_id=content_id,
            detection_method=detection_method,
            detection_date=date.today(),
            ai_probability=ai_probability,
            confidence_level=confidence_level,
            flagged_sections=flagged_sections,
            false_positive_risk=false_positive_risk
        )
        
        self.detection_results[detection_result.detection_id] = detection_result
        
        # Update usage record if it exists
        if actual_ai_usage:
            actual_ai_usage.detection_attempted = True
            actual_ai_usage.detected_as_ai = ai_probability > 0.5
            actual_ai_usage.detection_confidence = confidence_level
        
        return detection_result
    
    def implement_ai_policy(self, institution_id: str, policy_type: AIPolicy,
                           policy_description: str, allowed_tools: List[str] = None,
                           enforcement_level: float = 0.8) -> AIPolicyRecord:
        """Implement an AI policy for an institution."""
        if allowed_tools is None:
            allowed_tools = []
        
        # Default disclosure requirements by policy type
        disclosure_requirements = {
            AIPolicy.PROHIBITED: ["No AI use allowed"],
            AIPolicy.DISCLOSURE_REQUIRED: ["Must disclose all AI assistance", "Specify tools used", "Indicate content percentage"],
            AIPolicy.LIMITED_USE: ["Disclose AI use above 10% content", "Specify assistance type"],
            AIPolicy.UNRESTRICTED: ["Optional disclosure encouraged"],
            AIPolicy.UNDER_REVIEW: ["Temporary guidelines apply"]
        }
        
        # Default penalties by policy type
        violation_penalties = {
            AIPolicy.PROHIBITED: ["Academic misconduct investigation", "Paper retraction", "Disciplinary action"],
            AIPolicy.DISCLOSURE_REQUIRED: ["Warning for first offense", "Mandatory training", "Repeat offense sanctions"],
            AIPolicy.LIMITED_USE: ["Disclosure requirement", "Review of work"],
            AIPolicy.UNRESTRICTED: ["No penalties"],
            AIPolicy.UNDER_REVIEW: ["Case-by-case review"]
        }
        
        policy_record = AIPolicyRecord(
            institution_id=institution_id,
            policy_type=policy_type,
            effective_date=date.today(),
            policy_description=policy_description,
            allowed_tools=allowed_tools,
            disclosure_requirements=disclosure_requirements.get(policy_type, []),
            enforcement_level=enforcement_level,
            violation_penalties=violation_penalties.get(policy_type, [])
        )
        
        self.policy_records[institution_id] = policy_record
        
        # Update researcher profiles for this institution
        for profile in self.adoption_profiles.values():
            # Adjust compliance based on new policy
            if policy_type == AIPolicy.PROHIBITED:
                profile.adoption_rate *= 0.2  # Significant reduction
                profile.institutional_policy_compliance = enforcement_level
            elif policy_type == AIPolicy.DISCLOSURE_REQUIRED:
                profile.disclosure_compliance = min(1.0, profile.disclosure_compliance * (1 + enforcement_level * 0.5))
            elif policy_type == AIPolicy.LIMITED_USE:
                profile.adoption_rate *= 0.7  # Moderate reduction
            # UNRESTRICTED and UNDER_REVIEW don't change adoption significantly
        
        return policy_record
    
    def calculate_ai_impact_metrics(self) -> AIImpactMetrics:
        """Calculate aggregated AI impact metrics."""
        total_usage = len(self.usage_records)
        total_researchers = len(self.adoption_profiles)
        
        if total_usage == 0:
            return AIImpactMetrics(
                total_ai_usage_instances=0,
                ai_adoption_rate=0.0,
                average_content_percentage=0.0,
                disclosure_rate=0.0,
                detection_accuracy=0.0,
                quality_improvement_rate=0.0,
                policy_compliance_rate=0.0,
                tool_usage_distribution={},
                assistance_type_distribution={},
                detection_method_effectiveness={}
            )
        
        # Calculate adoption rate
        adopting_researchers = sum(1 for profile in self.adoption_profiles.values() 
                                 if profile.adoption_rate > 0.1)
        ai_adoption_rate = adopting_researchers / max(1, total_researchers)
        
        # Calculate average content percentage
        avg_content_pct = sum(record.content_percentage for record in self.usage_records.values()) / total_usage
        
        # Calculate disclosure rate
        disclosed_usage = sum(1 for record in self.usage_records.values() if record.is_disclosed)
        disclosure_rate = disclosed_usage / total_usage
        
        # Calculate detection accuracy
        detected_records = [record for record in self.usage_records.values() if record.detection_attempted]
        if detected_records:
            correct_detections = sum(1 for record in detected_records 
                                   if (record.detected_as_ai and record.content_percentage > 0.1) or
                                      (not record.detected_as_ai and record.content_percentage <= 0.1))
            detection_accuracy = correct_detections / len(detected_records)
        else:
            detection_accuracy = 0.0
        
        # Calculate quality improvement rate
        improved_records = sum(1 for record in self.usage_records.values()
                             if record.quality_impact in [AIQualityImpact.SIGNIFICANT_IMPROVEMENT,
                                                         AIQualityImpact.MODERATE_IMPROVEMENT,
                                                         AIQualityImpact.MINIMAL_IMPROVEMENT])
        quality_improvement_rate = improved_records / total_usage
        
        # Calculate policy compliance rate
        compliant_researchers = sum(1 for profile in self.adoption_profiles.values()
                                  if profile.institutional_policy_compliance > 0.7)
        policy_compliance_rate = compliant_researchers / max(1, total_researchers)
        
        # Tool usage distribution
        tool_distribution = {}
        for record in self.usage_records.values():
            tool_distribution[record.ai_tool_name] = tool_distribution.get(record.ai_tool_name, 0) + 1
        
        # Assistance type distribution
        type_distribution = {}
        for assistance_type in AIAssistanceType:
            type_distribution[assistance_type] = sum(1 for record in self.usage_records.values()
                                                   if record.assistance_type == assistance_type)
        
        # Detection method effectiveness
        method_effectiveness = {}
        for method in AIDetectionMethod:
            method_results = [result for result in self.detection_results.values()
                            if result.detection_method == method]
            if method_results:
                avg_confidence = sum(result.confidence_level for result in method_results) / len(method_results)
                method_effectiveness[method] = avg_confidence
            else:
                method_effectiveness[method] = 0.0
        
        return AIImpactMetrics(
            total_ai_usage_instances=total_usage,
            ai_adoption_rate=ai_adoption_rate,
            average_content_percentage=avg_content_pct,
            disclosure_rate=disclosure_rate,
            detection_accuracy=detection_accuracy,
            quality_improvement_rate=quality_improvement_rate,
            policy_compliance_rate=policy_compliance_rate,
            tool_usage_distribution=tool_distribution,
            assistance_type_distribution=type_distribution,
            detection_method_effectiveness=method_effectiveness
        )
    
    def simulate_policy_enforcement(self, institution_id: str, violation_threshold: float = 0.5) -> Dict[str, Any]:
        """Simulate enforcement of AI policies."""
        policy = self.policy_records.get(institution_id)
        if not policy:
            return {"error": "No policy found for institution"}
        
        # Find researchers from this institution
        institution_researchers = [profile for profile in self.adoption_profiles.values()
                                 if profile.institutional_policy_compliance < 1.0]  # Simplified institution matching
        
        violations = []
        enforcement_actions = []
        
        for profile in institution_researchers:
            researcher_usage = [record for record in self.usage_records.values()
                             if record.researcher_id == profile.researcher_id]
            
            for usage in researcher_usage:
                violation_detected = False
                violation_type = ""
                
                if policy.policy_type == AIPolicy.PROHIBITED and usage.content_percentage > 0:
                    violation_detected = True
                    violation_type = "Prohibited AI use"
                elif policy.policy_type == AIPolicy.DISCLOSURE_REQUIRED and not usage.is_disclosed:
                    violation_detected = True
                    violation_type = "Undisclosed AI use"
                elif policy.policy_type == AIPolicy.LIMITED_USE and usage.content_percentage > 0.3:
                    violation_detected = True
                    violation_type = "Excessive AI use"
                
                if violation_detected and random.random() < policy.enforcement_level:
                    violations.append({
                        "researcher_id": profile.researcher_id,
                        "usage_id": usage.usage_id,
                        "violation_type": violation_type,
                        "content_percentage": usage.content_percentage,
                        "disclosed": usage.is_disclosed
                    })
                    
                    # Apply enforcement action
                    if len(violations) == 1:  # First offense
                        action = "Warning issued"
                    elif len(violations) <= 3:  # Repeat offense
                        action = "Mandatory training required"
                    else:  # Serious violations
                        action = "Disciplinary review initiated"
                    
                    enforcement_actions.append({
                        "researcher_id": profile.researcher_id,
                        "action": action,
                        "date": date.today().isoformat()
                    })
        
        return {
            "institution_id": institution_id,
            "policy_type": policy.policy_type.value,
            "total_violations": len(violations),
            "violations": violations,
            "enforcement_actions": enforcement_actions,
            "compliance_rate": 1.0 - (len(violations) / max(1, len(institution_researchers)))
        }
    
    def get_researcher_ai_score(self, researcher_id: str) -> Dict[str, float]:
        """Calculate AI-related scores for a researcher."""
        profile = self.adoption_profiles.get(researcher_id)
        if not profile:
            return {"error": "Researcher profile not found"}
        
        researcher_usage = [record for record in self.usage_records.values()
                          if record.researcher_id == researcher_id]
        
        if not researcher_usage:
            return {
                "adoption_score": profile.adoption_rate,
                "disclosure_score": profile.disclosure_compliance,
                "quality_impact_score": 0.0,
                "policy_compliance_score": profile.institutional_policy_compliance,
                "overall_ai_score": (profile.adoption_rate + profile.disclosure_compliance + 
                                   profile.institutional_policy_compliance) / 3.0
            }
        
        # Calculate quality impact score
        quality_scores = {
            AIQualityImpact.SIGNIFICANT_IMPROVEMENT: 1.0,
            AIQualityImpact.MODERATE_IMPROVEMENT: 0.8,
            AIQualityImpact.MINIMAL_IMPROVEMENT: 0.6,
            AIQualityImpact.NO_CHANGE: 0.4,
            AIQualityImpact.QUALITY_DEGRADATION: 0.0
        }
        
        avg_quality_impact = sum(quality_scores[usage.quality_impact] for usage in researcher_usage) / len(researcher_usage)
        
        # Calculate actual disclosure rate
        disclosed_usage = sum(1 for usage in researcher_usage if usage.is_disclosed)
        actual_disclosure_rate = disclosed_usage / len(researcher_usage)
        
        overall_score = (
            profile.adoption_rate * 0.2 +
            actual_disclosure_rate * 0.3 +
            avg_quality_impact * 0.3 +
            profile.institutional_policy_compliance * 0.2
        )
        
        return {
            "adoption_score": profile.adoption_rate,
            "disclosure_score": actual_disclosure_rate,
            "quality_impact_score": avg_quality_impact,
            "policy_compliance_score": profile.institutional_policy_compliance,
            "overall_ai_score": overall_score
        }
    
    def _load_data(self):
        """Load existing AI impact data from files."""
        try:
            # Load usage records
            usage_file = self.data_dir / "usage_records.json"
            if usage_file.exists():
                with open(usage_file, 'r') as f:
                    data = json.load(f)
                    for record_data in data:
                        # Convert strings back to enums
                        record_data['assistance_type'] = AIAssistanceType(record_data['assistance_type'])
                        record_data['quality_impact'] = AIQualityImpact(record_data['quality_impact'])
                        record = AIUsageRecord(**record_data)
                        record.usage_date = datetime.strptime(record_data['usage_date'], '%Y-%m-%d').date()
                        self.usage_records[record.usage_id] = record
            
            # Load detection results
            detection_file = self.data_dir / "detection_results.json"
            if detection_file.exists():
                with open(detection_file, 'r') as f:
                    data = json.load(f)
                    for result_data in data:
                        # Convert string back to enum
                        result_data['detection_method'] = AIDetectionMethod(result_data['detection_method'])
                        result = AIDetectionResult(**result_data)
                        result.detection_date = datetime.strptime(result_data['detection_date'], '%Y-%m-%d').date()
                        self.detection_results[result.detection_id] = result
            
            # Load policy records
            policy_file = self.data_dir / "policy_records.json"
            if policy_file.exists():
                with open(policy_file, 'r') as f:
                    data = json.load(f)
                    for policy_data in data:
                        # Convert string back to enum
                        policy_data['policy_type'] = AIPolicy(policy_data['policy_type'])
                        policy = AIPolicyRecord(**policy_data)
                        policy.effective_date = datetime.strptime(policy_data['effective_date'], '%Y-%m-%d').date()
                        self.policy_records[policy.institution_id] = policy
            
            # Load adoption profiles
            profiles_file = self.data_dir / "adoption_profiles.json"
            if profiles_file.exists():
                with open(profiles_file, 'r') as f:
                    data = json.load(f)
                    for profile_data in data:
                        profile = AIAdoptionProfile(**profile_data)
                        self.adoption_profiles[profile.researcher_id] = profile
                        
        except Exception as e:
            # If loading fails, start with empty data
            pass
    
    def save_data(self):
        """Save AI impact data to files."""
        try:
            # Save usage records
            usage_data = []
            for record in self.usage_records.values():
                record_dict = asdict(record)
                record_dict['usage_date'] = record.usage_date.isoformat()
                # Convert enums to strings
                record_dict['assistance_type'] = record.assistance_type.value
                record_dict['quality_impact'] = record.quality_impact.value
                usage_data.append(record_dict)
            
            with open(self.data_dir / "usage_records.json", 'w') as f:
                json.dump(usage_data, f, indent=2)
            
            # Save detection results
            detection_data = []
            for result in self.detection_results.values():
                result_dict = asdict(result)
                result_dict['detection_date'] = result.detection_date.isoformat()
                # Convert enum to string
                result_dict['detection_method'] = result.detection_method.value
                detection_data.append(result_dict)
            
            with open(self.data_dir / "detection_results.json", 'w') as f:
                json.dump(detection_data, f, indent=2)
            
            # Save policy records
            policy_data = []
            for policy in self.policy_records.values():
                policy_dict = asdict(policy)
                policy_dict['effective_date'] = policy.effective_date.isoformat()
                # Convert enum to string
                policy_dict['policy_type'] = policy.policy_type.value
                policy_data.append(policy_dict)
            
            with open(self.data_dir / "policy_records.json", 'w') as f:
                json.dump(policy_data, f, indent=2)
            
            # Save adoption profiles
            profile_data = []
            for profile in self.adoption_profiles.values():
                profile_dict = asdict(profile)
                profile_data.append(profile_dict)
            
            with open(self.data_dir / "adoption_profiles.json", 'w') as f:
                json.dump(profile_data, f, indent=2)
                
        except Exception as e:
            raise PeerReviewError(f"Failed to save AI impact data: {str(e)}")